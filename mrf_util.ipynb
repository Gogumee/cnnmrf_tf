{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1601.04589 [Combining Markov Random Fields and Convolutional Neural Networks...]\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mrf_loss(style_layer, generated_layer, patch_size=3, name=''):\n",
    "    # type: (tf.Tensor, tf.Tensor, int, str) -> tf.Tensor\n",
    "    \"\"\"\n",
    "    :param style_layer: The vgg feature layer by feeding it the style image.\n",
    "    :param generated_layer: The vgg feature layer by feeding it the generated image.\n",
    "    :param patch_size: The patch size of the mrf.\n",
    "    :param name: Name scope of this loss.\n",
    "    :return: the mrf loss between the two inputted layers represented as a scalar tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Maybe I should make style_layer static to improve speed.\n",
    "    with tf.name_scope('mrf_loss' + name):\n",
    "        generated_layer_patches = create_local_patches(generated_layer, patch_size)\n",
    "        style_layer_patches = create_local_patches(style_layer, patch_size)\n",
    "        generated_layer_nn_matched_patches = patch_matching(generated_layer_patches, style_layer_patches, patch_size)\n",
    "        _, height, width, number = map(lambda i: i.value, generated_layer.get_shape())\n",
    "        size = height * width * number\n",
    "        # Normalize by the size of the image as well as the patch area.\n",
    "        loss = tf.div(tf.reduce_sum(tf.square(tf.sub(generated_layer_patches, generated_layer_nn_matched_patches))),\n",
    "                      size * (patch_size ** 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_local_patches(layer, patch_size, padding='VALID'):\n",
    "    # type: (tf.Tensor, int, str) -> tf.Tensor\n",
    "    \"\"\"\n",
    "    :param layer: Feature layer tensor with dimension (1, height, width, feature)\n",
    "    :param patch_size: The width and height of the patch. It is set to 3 in the paper https://arxiv.org/abs/1601.04589\n",
    "    :param padding: a string representing the padding style.\n",
    "    :return: Patches with dimension (cardinality, patch_size, patch_size, feature)\n",
    "    \"\"\"\n",
    "    return tf.extract_image_patches(layer, ksizes=[1, patch_size, patch_size, 1],\n",
    "                                    strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def patch_matching(generated_layer_patches, style_layer_patches, patch_size):\n",
    "    # type: (tf.Tensor, tf.Tensor, int) -> tf.Tensor\n",
    "    \"\"\"\n",
    "    The patch matching is implemented as an additional convolutional layer for fast computation.\n",
    "    In this case patches sampled from the style image are treated as the filters.\n",
    "    :param generated_layer_patches: Size (batch, height, width, patch_size * patch_size * feature)\n",
    "    :param style_layer_patches:Size (1, height, width, patch_size * patch_size * feature)\n",
    "    :param patch_size: the patch size for mrf.\n",
    "    :return: Best matching patch with size (batch, height, width, patch_size * patch_size * feature)\n",
    "    \"\"\"\n",
    "    # Every patch and every feature layer are treated as equally important after normalization.\n",
    "    normalized_generated_layer_patches = tf.nn.l2_normalize(generated_layer_patches, dim=[3])\n",
    "    normalized_style_layer_patches = tf.nn.l2_normalize(style_layer_patches, dim=[3])\n",
    "    # A better way to do this is to treat them as convolutions.\n",
    "    # They have to be in dimension\n",
    "    # (height * width, patch_size, patch_size, feature) <=> (batch, in_height, in_width, in_channels)\n",
    "    # (patch_size, patch_size, feature, height * width) <= > (filter_height, filter_width, in_channels, out_channels)\n",
    "    # Initially they are in [batch, out_rows, out_cols, patch_size * patch_size * depth]\n",
    "    original_shape = normalized_style_layer_patches.get_shape().as_list()\n",
    "    height = original_shape[1]\n",
    "    width = original_shape[2]\n",
    "    depth = original_shape[3] / patch_size / patch_size\n",
    "    normalized_style_layer_patches = tf.squeeze(normalized_style_layer_patches)\n",
    "\n",
    "    normalized_style_layer_patches = tf.reshape(normalized_style_layer_patches,\n",
    "                                                [height, width, patch_size, patch_size, depth])\n",
    "    normalized_style_layer_patches = tf.reshape(normalized_style_layer_patches,\n",
    "                                                [height * width, patch_size, patch_size, depth])\n",
    "    normalized_style_layer_patches = tf.transpose(normalized_style_layer_patches, perm=[1, 2, 3, 0])\n",
    "    style_layer_patches_reshaped = tf.reshape(style_layer_patches, [height, width, patch_size, patch_size, depth])\n",
    "    style_layer_patches_reshaped = tf.reshape(style_layer_patches_reshaped,\n",
    "                                              [height * width, patch_size, patch_size, depth])\n",
    "\n",
    "    normalized_generated_layer_patches_per_batch = tf.unpack(normalized_generated_layer_patches, axis=0)\n",
    "    ret = []\n",
    "    for batch in normalized_generated_layer_patches_per_batch:\n",
    "        original_shape = batch.get_shape().as_list()\n",
    "        height = original_shape[0]\n",
    "        width = original_shape[1]\n",
    "        depth = original_shape[2] / patch_size / patch_size\n",
    "        batch = tf.squeeze(batch)\n",
    "\n",
    "        batch = tf.reshape(batch, [height, width, patch_size, patch_size, depth])\n",
    "        batch = tf.reshape(batch, [height * width, patch_size, patch_size, depth])\n",
    "        # According to images-analogies github, for cross-correlation, we should flip the kernels\n",
    "        # That is normalized_style_layer_patches should be [:, ::-1, ::-1, :]\n",
    "        # I didn't see that in any other source, nor do I see why I should do so.\n",
    "        convs = tf.nn.conv2d(batch, normalized_style_layer_patches, strides=[1, 1, 1, 1], padding='VALID')\n",
    "        argmax = tf.squeeze(tf.argmax(convs, dimension=3))\n",
    "        # best_match has shape [height * width, patch_size, patch_size, depth]\n",
    "        best_match = tf.gather(style_layer_patches_reshaped, indices=argmax)\n",
    "        best_match = tf.reshape(best_match, [height, width, patch_size, patch_size, depth])\n",
    "        best_match = tf.reshape(best_match, [height, width, patch_size * patch_size * depth])\n",
    "        ret.append(best_match)\n",
    "    ret = tf.pack(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
